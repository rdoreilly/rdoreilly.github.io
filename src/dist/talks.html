<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Ruairí O'Reilly - Academic Profile</title>
    <link rel="shortcut icon" type="image/jpg" href="./img/favicon.ico">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.2/dist/umd/popper.min.js" integrity="sha384-IQsoLXl5PILFhosVNubq5LC7Qb9DXgDA9i+tQ8Zj3iwWAwPtgFTxbJ8NT4GN1R8p" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.min.js" integrity="sha384-cVKIPhGWiC2Al4u+LWgxfKTRIcfu0JTxR+EQDz/bgldoEyl4H0zUF0QKbrJ0EcQF" crossorigin="anonymous"></script>
    <style type="text/css">
      .fakeimg {
      	height: 200px;
      	background: #aaa;
      	}
    </style>
  </head>
</html>
<body>
  <div class="container">
    <nav class="navbar navbar-expand-sm bg-dark navbar-dark">
      <div class="container-fluid"><a class="navbar-brand" href="./index.html">Home</a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav me-auto mb-2 mb-lg-0">
            <li class="nav-item"><a class="nav-link" href="./teaching.html">Teaching</a></li>
            <li class="nav-item"><a class="nav-link" href="./research.html">Research</a></li>
            <li class="nav-item"><a class="nav-link" href="./pub.html">Publications</a></li>
            <li class="nav-item"><a class="nav-link" href="./about.html">About</a></li>
            <li class="nav-item"><a class="nav-link" href="./talks.html">Talks</a></li>
            <li class="nav-item"><a class="nav-link" href="./tools.html">Tools</a></li>
          </ul>
        </div>
      </div>
    </nav>
  </div>
  <div class="container" style="margin-top:30px">
    <div class="row">
      <div class="col-sm-4">
        <h4>Ruairí O'Reilly, Ph.D.</h4><img src="./img/headshot_ruairi.jpg" height="200">
        <p>Ruairí is a lecturer in the Department of Computer Science at Munster Technological University. Ruairí graduated with a BSc in Computer Science (2008) and a PhD in Computer Science (2015) from University College Cork. The focus if his research is on artificial intelligence, distributed systems, e-health and affective computing.</p>
        <h5>Connect</h5><a href="https://www.linkedin.com/in/rdoreilly/"><img src="img/linkedin.svg" alt="LinkedIn" style="height:36px;margin:1em .5em 1em .5em;"></a><a href="https://twitter.com/RuairiCS"><img src="img/twitter.svg" alt="@RuairiCS on Twitter" style="height:36px;margin:1em .5em 1em .5em;"></a>
        <h5>Collaborate</h5><a href="https://www.researchgate.net/profile/Ruairi_Oreilly2"><img src="img/researchgate.svg" alt="ResearchGate" style="height:36px;margin:1em .5em 1em .5em;"></a><a href="https://orcid.org/0000-0001-7990-3461"><img src="img/orcid.svg" alt="ORCID" style="height:36px;margin:1em .5em 1em .5em;"></a><a href="https://scholar.google.com/citations?user=86x5oQgAAAAJ&amp;hl=en"><img src="img/googlescholar.svg" alt="Google Scholar" style="height:36px;margin:1em .5em 1em .5em;"></a>
        <h5>Contact</h5><a href="mailto:ruairi.oreilly@mtu.ie"><img src="img/gmail.svg" alt="E-mail" style="height:36px;margin:1em .5em 1em .5em;"></a><a href="mailto:ruairi.oreilly@mtu.ie">ruairi.oreilly@mtu.ie</a>
      </div>
      <div class="col-sm-8">
        <h4>IEEE BHI-BSN 2021 Presentation - available now</h4>
        <h5>Methods for Creating Reproducible Machine Learning Pipelines for Skin Lesion Classification, August 5th 2021</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/0QDn4_B9lbM" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>Our work on a standardized approach for creating reproducible and sharable machine learning-based workflows for skin lesion classification as presented by David Walshe at the 2021 IEEE EMBS International Conference on Biomedical and Health Informatics. It combines a centralized data sourcing tool for the automated acquisition of highly cited skin lesion datasets from various online repositories. It provides a machine learning configuration-capture method that obtains a complete and faithful descriptor of a machine learning workflow. This descriptor is serialized into a sharable file format, enabling subsequent research to reimplement a cited model to a high degree of accuracy. The intent is to reduce the work associated with reimplementing state-of-the-art findings that arise from inconsistencies and ambiguities in recorded methodologies, increasing the velocity at which future research advancements can be achieved enabling a faithful reimplementation of baseline models.</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/353574080_Methods_for_Creating_Reproducible_Machine_Learning_Pipelines_for_Skin_Lesion_Classification">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=0QDn4_B9lbM">here</a>]. A packaged distribution of the data acq tool is available  [<a href="https://pypi.org/project/sla-cli/">here</a>].<br>
        <h4>IEEE BHI-BSN 2021 Presentation - available now</h4>
        <h5>Sleep Apnea Classification in an Activity Tracker Environment, August 5th 2021</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/tFdhm4YvTOA" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>Initial results on Sleep Apnea classification in a simulated Activity Tracker Environment as presented at 2021 IEEE EMBS International Conference on Biomedical and Health Informatics. The current generation of Activity trackers is becoming more advanced to derive blood oxygen saturation (SpO2), with SpO2 being an important indicator for sleep apnea. This work compares the performance of machine learning models in a simulated activity tracker environment. The results demonstrate that a Support Vector Machine (SVM) classifier trained with features extracted from a Convolutional Neural Network (CNN) can classify sleep apnea with a high degree of accuracy, warranting further investigation.</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/353572505_Sleep_Apnea_Classification_in_an_Activity_Tracker_Environment">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=tFdhm4YvTOA&amp;feature=youtu.be">here</a>]. [<a href="">here</a>].<br>
        <h4>ISSC 2021 - Presentation by Zach Dair</h4>
        <h5>Machine Learning Classification Emotive Speech Expression, June 11th 2021</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/RxV6EhklSOo" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>Our work on emotive expression classification through speech analysis was presented at ISSC2021 by Zachary Dair. The approach combined affective prosody (Mel-frequency Cepstral Coefficient, Zero Crossing Rate, Chroma Energy Normalised) and semantic analysis (Bag-of-words model). A Convolutional neural network and Logistic regression model were combined to form an ensemble-based approach for the classification of emotive expressions from multi-modal data (audio, text). The approach builds upon existing work in emotion classification, sentiment analysis, and natural language processing techniques. Results demonstrate mixed accuracy across varied data sources, indicating the limitations and considerations of a generalised approach. There are direct benefits for Affective computing research as it enables (a) insight into the strengths and limitations of such models in correctly classifying emotion in relation to population differences (e.g. gender) and provides (b) a baseline for emotion classification in speech across the six canonical basic emotions (Anger, Fear, Disgust, Joy, Sadness, Surprise).</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/352926496_Classification_of_Emotive_Expression_Using_Verbal_and_Non_Verbal_Components_of_Speech">here</a>].</p>A video presentation is available [<a href="https://youtu.be/RxV6EhklSOo">here</a>]. [<a href="">here</a>].<br>
        <h4>IEEE CBMS 2021 - Urja Pawar</h4>
        <h5>Evaluating Hierarchical Medical Workflows using Feature Importance, June 9th 2021</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/bhNvW88EWvQ" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>Our work on utilising hierarchical medical workflow for understanding the operation of ML in a healthcare-based setting was presented by Urja Pawar at IEEE CBMS 2021. The utility of the approach is demonstrated in the context of heart disease classification. Explainable Artificial Intelligence (XAI) is incorporated in the form of Feature Importance scores and correlated with an ML model's performance metrics (Accuracy, F1-score). This provides a multi-stakeholder perspective aligned with the hierarchy as experienced in a real-world medical setting. The work contributes a methodology for accommodating an enhanced understanding of diverse hierarchical healthcare settings that would benefit from the adoption of AI-based systems.</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/353197756_Evaluating_Hierarchical_Medical_Workflows_using_Feature_Importance">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=bhNvW88EWvQ">here</a>]. [<a href="">here</a>].<br>
        <h4>HUCAPP2021 - Conference Presentation by Ryan Donovan</h4>
        <h5>A Multimodal Workflow for Modeling Personality And Emotions to Enable User Profiling and Personalisation, February 9th 2021</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/EgHgyeAnVI8" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p> Introducing PEM - A Multimodal Workflow for Modelling Personality And Emotions to Enable User Profiling and Personalisation. Ryan Donovan presents PEM’s functionality and use cases as part of #HUCAPP2021. Multimodal analysis for state-to-trait mappings updated with regard to magnitude, direction, and statistical significance as data is entered. Will appeal to those interested in #affectivecomputing #appliedpsychology Camera-ready paper is now available on Research Gate.</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/349179575_A_Multimodal_Workflow_for_Modeling_Personality_And_Emotions_to_Enable_User_Profiling_and_Personalisation">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=EgHgyeAnVI8">here</a>]. [<a href="">here</a>].<br>
        <h4>AICS 2020</h4>
        <h5>Differentiation in Personality Emotion Mappings From Self Reported Emotion and Automatically Classified Emotion, December 7th 2020</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/Dj7pyyBVK8w" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>PhD student Ryan Donovan on differentiation in #Personality and #Emotion Mappings From Self-Reported Emotion and Automatically Classified Emotion presented as part of #AICS2020. The work investigates the robustness of the relationship between personality and emotions across modalities. It is a step towards enabling a more generalised bidirectional model of personality-emotion mapping which will be of interest to researchers in the area of #affectivecomputing and #appliedpsychology The camera-ready paper is now available on Research Gate - https://bit.ly/37sav9n</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/346717964_Differentiation_in_Personality_Emotion_Mappings_From_Self_Reported_Emotion_and_Automatically_Classified_Emotion">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=Dj7pyyBVK8w">here</a>]. [<a href="">here</a>].<br>
        <h4>AICS 2020</h4>
        <h5>Incorporating Explainable Artificial Intelligence (XAI) to aid the Understanding of Machine Learning in the Healthcare Domain, December 7th 2020</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/c1kzrRc_sXA" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>PhD student Urja Pawar on Incorporating Explainable Artificial Intelligence (XAI) to aid the understanding of Machine Learning in the Healthcare Domain presented as part of #AICS2020. The camera-ready paper now available on Research Gate</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/346717871_Incorporating_Explainable_Artificial_Intelligence_XAI_to_aid_the_Understanding_of_Machine_Learning_in_the_Healthcare_Domain">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=c1kzrRc_sXA">here</a>]. [<a href="">here</a>].<br>
        <h4>IMVIP 2020</h4>
        <h5>Performance Analysis of State-of-the-Art CNN versus a Capsule Network for Cell Image Classification, 31st August 2020</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/z_b0AprdLZI?start=0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>Despite the state of the art performance on object recognition and image classification problems, CNNs are considered to have two significant weaknesses. Firstly, their inability to cater to changes in object orientation, position or lighting. Secondly, their inability to deal with part-whole relationships between objects. Capsule Networks are an enhancement to CNNs to more closely model the viewpoint invariance capability of human vision. The application of Capsule Networks to well-known datasets, such as MNIST and NORB, has achieved a state of the art performance, while the application to other datasets has had mixed results. The application of Capsule Networks to domains such as medical-based imaging problems is of significant interest as they have been shown to train accurately on some datasets with limited training data. Research undertaken by Liam Murphy (MSc AI) on the Performance of a State-of-the-Art CNN versus a Capsule Network for Cell Image Classification presented as part of the Irish Machine Vision & Image Processing Conference #IMVIP2020</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/344042311_A_Performance_Analysis_of_a_State_of_the_Art_Convolutional_Neural_Network_versus_a_Capsule_Network_for_Cell_Image_Classification">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=z_b0AprdLZI">here</a>]. [<a href="">here</a>].<br>
        <h4>IMVIP 2020</h4>
        <h5>An Ensemble-based Approach to the Detection of COVID-19 Induced Pneumonia using X-Ray Imagery, 31st August 2020</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/5bZcyg80g0c" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>AThe rapid emergence and spread of COVID-19 resulted in a surge in demand for laboratory-based testing globally. Currently, the gold standard diagnostic approach is large-scale molecular testing of biological samples which detect the SARS-CoV-2 virus RNA.  Infrastructure limitations and supply shortages are limiting testing capacity with a growing demand for COVID-19 diagnostics across the EU. X-ray imagery is essential in establishing the severity of a multitude of diseases and monitoring responses of patients in the hospital setting. X-ray imagery should not be used to screen for or as a first-line test to detect COVID-19. However, X-ray presents an ideal opportunity to integrate additional screening measures into a pre-existing workflow. This paper investigates the utilisation of machine learning in automating the detection of COVID-19 induced pneumonia from X-Ray imagery.The approach will assist radiologists in the monitoring and differentiation of pneumonia caused by COVID-19 from other viral causes. A classification for the presence, or absence, of pneumonia caused by COVID-19 and other viral causes is derived. The paper contributes an initial investigation into an ensemble-learning based approach using transfer learning models VGG16, Inception and ResNet. The results of this work indicate an improved performance using ensemble-based learning as compared to an individual transfer learning model.</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/344042405_An_Ensemble-based_Approach_to_the_Detection_of_COVID-19_Induced_Pneumonia_using_X-Ray_Imagery">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=5bZcyg80g0c&amp;feature=youtu.be">here</a>]. [<a href="">here</a>].<br>
        <h4>Business Post - article</h4>
        <h5>How can #AI lead the way in fighting COVID19?, 28th August 2020</h5><br>
        <p>Op-ed piece on recent work undertaken by Farshad Ghassemi Toosi and myself in the Business Post over the weekend discussing how Artificial Intelligence can be used to predict which Coronavirus infected patient may develop severe respiratory symptoms.</p>The full article is available [<a href="https://www.businesspost.ie/ai-data/how-ai-can-lead-the-way-in-battle-against-covid-19-ad18d736">here</a>].<br>
        <h4>ICCSA 2020</h4>
        <h5>Quantifying the Links between Personality Sub-Traits and the Basic Emotions, July 4th 2020</h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/CpAT67Qom-Y" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>This video presents an exploratory study that aimed to analyse the relationship between personality traits and emotions. In particular, it investigates to what extent the sub-traits of the Five-Factor Model has an empirically quantifiable correlation with the Basic Emotions (Anger, Anxiety, Disgust, Fear, Joy, Sadness, Surprise). If links between these personality traits and the basic emotions can be found, then this would enable an emotional-state-to-personality-trait mapping. In this study, 38 participants answered a Big Five Aspects Scale (BFAS) questionnaire and then watched 12 emotionally provocative film clips along with answering 12 short emotional Likert-scales on their emotional experiences during each film clip. The results showed that (i) four of the seven Basic Emotions outright significantly correlated, while two emotions (Fear and Disgust) approached statistical significance,  with at least one of the personality traits and (ii) significant correlations between personality traits and basic emotions could only be identified at the sub-trait level, demonstrating the value in adopting a higher-resolution personality model. The results support the long-term goal of this research, which is the enabling of state-to-trait inferences. A method for analysing and visualising such a mapping, that differentiates mappings based on the direction and magnitude of the effect size was also developed. The study contributes a blueprint towards utilising Affective Computing methodology to automatically map these phenomena.</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/343691819_Quantifying_the_Links_between_Personality_Sub-Traits_and_the_Basic_Emotions">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=CpAT67Qom-Y">here</a>]. [<a href="">here</a>].<br>
        <h4>CyberScience 2020</h4>
        <h5>Explainable Artificial intelligence in Healthcare, </h5><br>
        <div class="text-center">
          <div class="ratio ratio-16x9" style="max-width: 80%; margin: auto;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/FJQaoeQtPks" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <p>Explainable Artificial intelligence in Healthcare. Artificial Intelligence (AI) is an enabling technology that when integrated into healthcare applications and smart wearable devices such as Fitbits etc. can predict the occurrence of health conditions in users by capturing and analysing their health data. The integration of AI and smart wearable devices has a range of potential applications in the area of smart healthcare but there is a challenge in the black box operation of decisions made by AI models which have resulted in a lack of accountability and trust in the decisions made. Explainable AI (XAI) is a domain in which techniques are developed to explain predictions made by AI systems. In this paper, XAI is discussed as a technique that can be used in the analysis and diagnosis of health data by AI-based systems and a proposed approach presented with the aim of achieving accountability. transparency, result tracing, and model improvement in the domain of healthcare.</p>
        <p>The camera-ready pre-print is available [<a href="https://www.researchgate.net/publication/342600571_Explainable_AI_in_Healthcare">here</a>].</p>A video presentation is available [<a href="https://www.youtube.com/watch?v=FJQaoeQtPks&amp;feature=share">here</a>]. [<a href="">here</a>].<br>
      </div>
    </div>
  </div>
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-sm-4"><img src="https://img.shields.io/github/followers/rdoreilly?style=social"></div>
        <div class="col-sm-4"><img src="https://img.shields.io/twitter/follow/RuairiCS?style=social"></div>
        <div class="col-sm-4"><img src="https://img.shields.io/youtube/channel/subscribers/UCd-mjrMlcYUf2MpyEo4Nd-g?style=social"></div>
      </div>
    </div>
  </footer>
</body>